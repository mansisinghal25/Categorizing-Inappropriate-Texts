{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mansi-SVM.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Normal SVM "],"metadata":{"id":"l5xt6_qAGOY2"}},{"cell_type":"code","source":["!pip install scikit-multilearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmQPwEhbQzmi","outputId":"db2bd733-c1d8-422e-d054-e368eaf68882","executionInfo":{"status":"ok","timestamp":1649829926462,"user_tz":-330,"elapsed":4191,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-multilearn\n","  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n","\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 26.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 36.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40 kB 17.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 89 kB 6.1 MB/s \n","\u001b[?25hInstalling collected packages: scikit-multilearn\n","Successfully installed scikit-multilearn-0.2.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yEuwaI_FnUM","outputId":"4a92edd7-98f2-4024-93d3-818057f42d21","executionInfo":{"status":"ok","timestamp":1649829932709,"user_tz":-330,"elapsed":6253,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk.corpus\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords, words\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import scale, MinMaxScaler\n","from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import learning_curve\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.multioutput import MultiOutputClassifier"]},{"cell_type":"code","source":["test_label = pd.read_csv(\"test_labels.csv\")\n","test = pd.read_csv(\"test.csv\")\n","train = pd.read_csv(\"train.csv\")\n","# Merging test and train to form one huge dataset\n","test_data = pd.merge(test, test_label)\n","dataset = pd.concat([test_data, train])\n","dataset.drop(columns=['id'], inplace=True)\n","dataset.drop_duplicates(inplace=True, ignore_index=True)\n","dataset.drop(dataset.index[dataset['toxic'] == -1], inplace = True)\n","dataset.reset_index(inplace = True)\n","# Text cleaning\n","#converting to lower case\n","dataset['comment_text_cleaned'] = dataset['comment_text'].str.lower()\n","#removing special characters\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", str(elem)))\n","#removing numbers\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"\\d+\", \"\", str(elem)))\n","# Removing stop words\n","stop = stopwords.words('english')\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","#Tokenizing\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_tokenize(x))\n","#Lemmitization\n","def word_lemmatizer(text):\n","    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n","    return lem_text\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_lemmatizer(x))\n","# Splitting into train test sets\n","X = dataset.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n","y = dataset[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].copy()\n","\n","X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X,y, train_size=0.8)\n","X_val, X_test, y_val, y_test = train_test_split(X_test_and_val,y_test_and_val, train_size=0.5)\n","train_tokens = pd.Series(X_train['comment_text_cleaned']).values\n","w2v_model = Word2Vec(train_tokens, size= 200)\n","def buildWordVector(tokens, size):\n","  vec = np.zeros(size).reshape((1, size))\n","  count = 0.\n","  for word in tokens:\n","    try:\n","      vec += w2v_model[word].reshape((1, size))\n","      count += 1.\n","    except KeyError:\n","      continue\n","  if count != 0:\n","    vec /= count\n","  return vec\n","\n","train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in train_tokens])\n","# train_vecs_w2v = scaler.fit_transform(train_vecs_w2v)\n","train_vecs_w2v = scale(train_vecs_w2v)\n","\n","val_tokens = pd.Series(X_val['comment_text_cleaned']).values\n","val_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in val_tokens])\n","# val_vecs_w2v = scaler.transform(val_vecs_w2v)\n","val_vecs_w2v = scale(val_vecs_w2v)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cFzkBa1GtU2","outputId":"5bdffbc7-286b-405d-b8fa-d3439ec5c179","executionInfo":{"status":"ok","timestamp":1649830238222,"user_tz":-330,"elapsed":305519,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"]}]},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","#classifier.fit(train_vecs_w2v, y_train)\n","#predictions = classifier.predict(val_vecs_w2v)\n","#train_pred = classifier.predict(train_vecs_w2v)\n","\n","\n","\n","parameters = [\n","    {\n","        'classifier': [SVC()],\n","        'classifier__kernel': ['rbf', 'linear', 'poly'],\n","        'classifier__C': [0.1, 1, 10, 100, 1000],\n","        'classifier__gamma': [0.1, 1, 10, 100],\n","        'classifier__degree': [0, 1, 2, 3, 4, 5, 6]\n","    }\n","]\n","\n","clf = RandomizedSearchCV(ClassifierChain(), parameters, scoring=\"f1_micro\", return_train_score=True)\n","clf.fit(train_vecs_w2v, y_train)\n","clf.best_estimator_.fit(train_vecs_w2v, y_train)"],"metadata":{"id":"Z_g9Fg2eHHJb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluation_metric(model_name, feature_extraction, y_true, y_pred):\n","  print('Model:', model_name)\n","  print('Feature extraction method:', feature_extraction)\n","  recall= recall_score(y_true, y_pred, average='micro')\n","  prec = precision_score(y_true, y_pred, average='micro')\n","  final_score = recall*0.6 + prec*0.4\n","  print('Precision: ', prec)\n","  print('Recall: ', recall)\n","  print('Final score of the model: ', final_score)\n","  return final_score"],"metadata":{"id":"DDBOnQDWHQPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Optimal hyperparameter combination:\", clf.best_params_)\n","print()\n","print(\"Mean cross-validated training accuracy score:\", clf.best_score_)\n","predictions = clf.best_estimator_.predict(val_vecs_w2v) # Predictions\n","predictions_train = clf.best_estimator_.predict(train_vecs_w2v)\n","\n","result_test = evaluation_metric('Support Vector Machine with Classifier Chains', 'Word2Vec', y_val, predictions)\n","result_train = evaluation_metric('Support Vector Machine with Classifier Chains', 'Word2Vec', y_train, predictions_train)\n","print(\"Variance is: \",result_train - result_test)"],"metadata":{"id":"Vb7VoBeB_Rkd"},"execution_count":null,"outputs":[]}]}