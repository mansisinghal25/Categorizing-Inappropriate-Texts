{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aditi-RF_random_Search.ipynb","provenance":[{"file_id":"1O0to8PBWlKoaWD7qhU53EsU7rDeDt0j0","timestamp":1649691893223},{"file_id":"1e_znxowb0nLfQGlJ78EeNXMALmS8nu_n","timestamp":1649691377297}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install scikit-multilearn"],"metadata":{"id":"DmQPwEhbQzmi","executionInfo":{"status":"ok","timestamp":1649829963151,"user_tz":-330,"elapsed":7619,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"0daeb863-66a3-4adf-d899-0c5d44d2d8f6","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9yEuwaI_FnUM","executionInfo":{"status":"ok","timestamp":1649829963152,"user_tz":-330,"elapsed":20,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"ded15dbe-58d7-4737-fd6d-4d5b45b6272f"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk.corpus\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords, words\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import scale, MinMaxScaler\n","from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import learning_curve\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.model_selection import RandomizedSearchCV"]},{"cell_type":"markdown","source":["# Preprocessing + Word2Vec"],"metadata":{"id":"NCUglHd948jP"}},{"cell_type":"code","source":["test_label = pd.read_csv(\"test_labels.csv\")\n","test = pd.read_csv(\"test.csv\")\n","train = pd.read_csv(\"train.csv\")\n","# Merging test and train to form one huge dataset\n","test_data = pd.merge(test, test_label)\n","dataset = pd.concat([test_data, train])\n","dataset.drop(columns=['id'], inplace=True)\n","dataset.drop_duplicates(inplace=True, ignore_index=True)\n","dataset.drop(dataset.index[dataset['toxic'] == -1], inplace = True)\n","dataset.reset_index(inplace = True)\n","# Text cleaning\n","#converting to lower case\n","dataset['comment_text_cleaned'] = dataset['comment_text'].str.lower()\n","#removing special characters\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", str(elem)))\n","#removing numbers\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"\\d+\", \"\", str(elem)))\n","# Removing stop words\n","stop = stopwords.words('english')\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","#Tokenizing\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_tokenize(x))\n","#Lemmitization\n","def word_lemmatizer(text):\n","    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n","    return lem_text\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_lemmatizer(x))\n","# Splitting into train test sets\n","X = dataset.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n","y = dataset[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].copy()\n","\n","X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X,y, train_size=0.8)\n","X_val, X_test, y_val, y_test = train_test_split(X_test_and_val,y_test_and_val, train_size=0.5)\n","train_tokens = pd.Series(X_train['comment_text_cleaned']).values\n","w2v_model = Word2Vec(train_tokens, size= 200)\n","def buildWordVector(tokens, size):\n","  vec = np.zeros(size).reshape((1, size))\n","  count = 0.\n","  for word in tokens:\n","    try:\n","      vec += w2v_model[word].reshape((1, size))\n","      count += 1.\n","    except KeyError:\n","      continue\n","  if count != 0:\n","    vec /= count\n","  return vec\n","\n","train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in train_tokens])\n","train_vecs_w2v = scale(train_vecs_w2v)\n","\n","val_tokens = pd.Series(X_val['comment_text_cleaned']).values\n","val_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in val_tokens])\n","val_vecs_w2v = scale(val_vecs_w2v)"],"metadata":{"id":"2cFzkBa1GtU2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649830266264,"user_tz":-330,"elapsed":303124,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"14d3b313-77f8-48f9-dbba-be74315b3477"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"]}]},{"cell_type":"code","source":["def evaluation_metric(model_name, feature_extraction, y_true, y_pred):\n","  print('Model:', model_name)\n","  print('Feature extraction method:', feature_extraction)\n","  recall= recall_score(y_true, y_pred, average='micro')\n","  prec = precision_score(y_true, y_pred, average='micro')\n","  final_score = recall*0.6 + prec*0.4\n","  print('Precision: ', prec)\n","  print('Recall: ', recall)\n","  print('Final score of the model: ', final_score)\n","  return final_score"],"metadata":{"id":"DDBOnQDWHQPs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hyperparameter_space = {'n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n","                        'criterion':['gini','entropy'],\n","                        'max_depth': np.linspace(1, 20, 10, endpoint=True),\n","                        'min_samples_leaf': np.linspace(0.1, 0.5, 5, endpoint=True),\n","                        'min_samples_split': np.linspace(0.1, 1.0, 10, endpoint=True),\n","                        'max_features': ['auto','sqrt','log2']}\n","\n","parameters = {\n","      'estimator__n_estimators': [1, 2, 4, 8, 16, 32, 64, 100, 200],\n","      'estimator__criterion': ['gini','entropy'],\n","      'estimator__max_depth': np.linspace(1, 32, 32, endpoint=True),\n","      'estimator__max_features': ['auto','sqrt','log2'],\n","    }"],"metadata":{"id":"mkZmxVmgUxQR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["clf = MultiOutputClassifier(RandomForestClassifier())\n","rs = RandomizedSearchCV(clf, param_distributions=parameters, scoring=\"f1_micro\", return_train_score=True)\n","rs.fit(train_vecs_w2v, y_train)\n","rs.best_estimator_.fit(train_vecs_w2v, y_train)"],"metadata":{"id":"iT8jIzN5UUwW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649830603044,"user_tz":-330,"elapsed":336793,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"7ef14dbc-53b5-43e2-d92c-7a3ec6d52531"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultiOutputClassifier(estimator=RandomForestClassifier(criterion='entropy',\n","                                                       max_depth=16.0,\n","                                                       max_features='log2',\n","                                                       min_samples_leaf=0.5,\n","                                                       min_samples_split=0.1,\n","                                                       n_estimators=16))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["print(\"Optimal hyperparameter combination:\", rs.best_params_)\n","print()\n","print(\"Mean cross-validated training accuracy score:\", rs.best_score_)\n","predictions = rs.best_estimator_.predict(val_vecs_w2v) # Predictions\n","predictions_train = rs.best_estimator_.predict(train_vecs_w2v)\n","\n","result_test = evaluation_metric('Random Forest with Multi Output classifier', 'Word2Vec', y_val, predictions)\n","result_train = evaluation_metric('Random Forest with Multi Output classifier', 'Word2Vec', y_train, predictions_train)\n","print(\"Variance is: \",result_train - result_test)"],"metadata":{"id":"SYf_vXShlQvf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649830604780,"user_tz":-330,"elapsed":1748,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"bad9cbbd-f943-419e-ad85-0fe060eb6ac8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal hyperparameter combination: {'estimator__n_estimators': 16, 'estimator__min_samples_split': 0.1, 'estimator__min_samples_leaf': 0.5, 'estimator__max_features': 'log2', 'estimator__max_depth': 16.0, 'estimator__criterion': 'entropy'}\n","\n","Mean cross-validated training accuracy score: 0.0\n","Model: Random Forest with Multi Output classifier\n","Feature extraction method: Word2Vec\n","Precision:  0.0\n","Recall:  0.0\n","Final score of the model:  0.0\n","Model: Random Forest with Multi Output classifier\n","Feature extraction method: Word2Vec\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Precision:  0.0\n","Recall:  0.0\n","Final score of the model:  0.0\n","Variance is:  0.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}]}