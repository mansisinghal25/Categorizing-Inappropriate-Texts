{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7461,"status":"ok","timestamp":1652608046550,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"},"user_tz":-330},"id":"NJOJtH6_TVSY","outputId":"caebd96c-1507-4b98-928c-e9868403c254"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting scikit-multilearn\n","  Downloading scikit_multilearn-0.2.0-py3-none-any.whl (89 kB)\n","\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 17.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 40 kB 9.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 89 kB 4.1 MB/s \n","\u001b[?25hInstalling collected packages: scikit-multilearn\n","Successfully installed scikit-multilearn-0.2.0\n"]}],"source":["!pip install scikit-multilearn"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6441,"status":"ok","timestamp":1652608052988,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"},"user_tz":-330},"id":"7MRbq9u6vqON","outputId":"1b0b7a81-a55c-4a33-ebed-15eebacb5716"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk.corpus\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords, words\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import scale, MinMaxScaler\n","from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import learning_curve\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.multioutput import MultiOutputClassifier\n","from skmultilearn.model_selection import iterative_train_test_split"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304923,"status":"ok","timestamp":1652610438693,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"},"user_tz":-330},"id":"GL8T2BwKEUNB","outputId":"f30e6021-f41a-4d39-9c69-711cf5c68607"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"]}],"source":["test_label = pd.read_csv(\"test_labels.csv\")\n","test = pd.read_csv(\"test.csv\")\n","train = pd.read_csv(\"train.csv\")\n","# Merging test and train to form one huge dataset\n","test_data = pd.merge(test, test_label)\n","dataset = pd.concat([test_data, train])\n","dataset.drop(columns=['id'], inplace=True)\n","dataset.drop_duplicates(inplace=True, ignore_index=True)\n","dataset.drop(dataset.index[dataset['toxic'] == -1], inplace = True)\n","dataset.reset_index(inplace = True)\n","# Text cleaning\n","#converting to lower case\n","dataset['comment_text_cleaned'] = dataset['comment_text'].str.lower()\n","#removing special characters\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", str(elem)))\n","#removing numbers\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"\\d+\", \"\", str(elem)))\n","# Removing stop words\n","stop = stopwords.words('english')\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","#Tokenizing\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_tokenize(x))\n","#Lemmitization\n","def word_lemmatizer(text):\n","    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n","    return lem_text\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_lemmatizer(x))\n","# Splitting into train test sets\n","X = dataset.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n","y = dataset[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].copy()\n","\n","X_new, X_discard, y_new, y_discard = train_test_split(X, y, test_size = 0.95)\n","X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X_discard, y_discard, train_size=0.8)\n","X_val, X_test, y_val, y_test = train_test_split(X_test_and_val,y_test_and_val, train_size=0.5)\n","train_tokens = pd.Series(X_train['comment_text_cleaned']).values\n","w2v_model = Word2Vec(train_tokens, size= 200)\n","def buildWordVector(tokens, size):\n","  vec = np.zeros(size).reshape((1, size))\n","  count = 0.\n","  for word in tokens:\n","    try:\n","      vec += w2v_model[word].reshape((1, size))\n","      count += 1.\n","    except KeyError:\n","      continue\n","  if count != 0:\n","    vec /= count\n","  return vec\n","\n","train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in train_tokens])\n","train_vecs_w2v = scale(train_vecs_w2v)\n","\n","val_tokens = pd.Series(X_val['comment_text_cleaned']).values\n","val_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in val_tokens])\n","val_vecs_w2v = scale(val_vecs_w2v)\n","\n","def evaluation_metric(model_name, feature_extraction, y_true, y_pred):\n","  print('Model:', model_name)\n","  print('Feature extraction method:', feature_extraction)\n","  recall= recall_score(y_true, y_pred, average='micro')\n","  prec = precision_score(y_true, y_pred, average='micro')\n","  final_score = recall*0.6 + prec*0.4\n","  print('Precision: ', prec)\n","  print('Recall: ', recall)\n","  print('Final score of the model: ', final_score)\n","  return final_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fVpDKcXgEb-t"},"outputs":[],"source":["from sklearn.svm import SVC\n","parameters = [\n","    {\n","        'classifier' : [SVC()],\n","        'classifier__C': [0.1, 1, 10, 100, 1000], \n","        'classifier__kernel': ['linear'],\n","    },\n","    {\n","        'classifier' : [SVC()],\n","        'classifier__C': [0.1, 1, 10, 100, 1000], \n","        'classifier__gamma': [0.001, 0.0001, 0.1, 1, 10, 100],\n","        'classifier__kernel': ['rbf', 'sigmoid'],\n","    },\n","    {\n","        'classifier' : [SVC()],\n","        'classifier__C': [0.1, 1, 10, 100, 1000], \n","        'classifier__degree': [0, 1, 2, 3, 4, 5, 6],\n","        'classifier__kernel': ['poly'],\n","    } \n","]\n","\n","clf = RandomizedSearchCV(ClassifierChain(), parameters, scoring=\"f1_micro\", return_train_score=True)\n","clf.fit(train_vecs_w2v, y_train)\n","clf.best_estimator_.fit(train_vecs_w2v, y_train)\n","\n","# {\n","#         'classifier' : [LogisticRegression()],\n","#         'classifier__penalty' : ['l2'],\n","#         'classifier__C' : np.logspace(-4, 4, 20),\n","#         'classifier__solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n","#         'classifier__max_iter': [20, 50, 100, 200, 500, 1000],\n","#     }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DPI3XgLvJPm7"},"outputs":[],"source":["print(\"Optimal hyperparameter combination:\", clf.best_params_)\n","print()\n","print(\"Mean cross-validated training accuracy score:\", clf.best_score_)\n","predictions = clf.best_estimator_.predict(val_vecs_w2v) # Predictions\n","predictions_train = clf.best_estimator_.predict(train_vecs_w2v)\n","\n","result_test = evaluation_metric('Tuned SVM with Classifier Chains', 'Word2Vec', y_val, predictions)\n","result_train = evaluation_metric('Tuned SVM with Classifier Chains', 'Word2Vec', y_train, predictions_train)\n","print(\"Variance is: \",result_train - result_test)"]},{"cell_type":"markdown","source":["## SVM with Label Powerset"],"metadata":{"id":"AmDMBiCZM6W6"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","classifier = LabelPowerset(SVC())\n","classifier.fit(train_vecs_w2v, y_train)\n","predictions = classifier.predict(val_vecs_w2v)\n","train_pred = classifier.predict(train_vecs_w2v)"],"metadata":{"id":"DStgIZDvBcGp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluation_metric(model_name, feature_extraction, y_true, y_pred):\n","  print('Model:', model_name)\n","  print('Feature extraction method:', feature_extraction)\n","  recall= recall_score(y_true, y_pred, average='micro')\n","  prec = precision_score(y_true, y_pred, average='micro')\n","  final_score = recall*0.6 + prec*0.4\n","  print('Precision: ', prec)\n","  print('Recall: ', recall)\n","  print('Final score of the model: ', final_score)\n","  return final_score\n","\n","result_test = evaluation_metric('SVM with label powerset', 'Word2Vec', y_val, predictions)\n","print()\n","result_train = evaluation_metric('SVM with label powerset', 'Word2Vec', y_train, train_pred)\n","print()\n","print(\"Variance is: \",result_train - result_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp6rU7DsCMEi","executionInfo":{"status":"ok","timestamp":1649937762016,"user_tz":-330,"elapsed":1449,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"f2cc08ce-7edf-4229-c3fd-aea7c663409d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: SVM with label powerset\n","Feature extraction method: Word2Vec\n","Precision:  0.8517975055025678\n","Recall:  0.45925632911392406\n","Final score of the model:  0.6162727996693815\n","\n","Model: SVM with label powerset\n","Feature extraction method: Word2Vec\n","Precision:  0.8974793767186068\n","Recall:  0.4948576049326561\n","Final score of the model:  0.6559063136470364\n","\n","Variance is:  0.03963351397765491\n"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Mansi-SVM_random_search.ipynb","provenance":[{"file_id":"13QKncdp5AXcP65KI_PbqO5-n7wjRpYfg","timestamp":1649016327423}],"authorship_tag":"ABX9TyM5Vu53EpTq0+bTrAEqgcWH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}