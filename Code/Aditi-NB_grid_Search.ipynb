{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Aditi-NB_grid_Search.ipynb","provenance":[{"file_id":"1MxtpfkNsoaAfj8itt-qdItiFVB1Q1TZk","timestamp":1649027270958},{"file_id":"13QKncdp5AXcP65KI_PbqO5-n7wjRpYfg","timestamp":1649016327423}],"collapsed_sections":[],"authorship_tag":"ABX9TyMTyxbYcWBg/norXJgNsL+W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install scikit-multilearn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJOJtH6_TVSY","executionInfo":{"status":"ok","timestamp":1649789389920,"user_tz":-330,"elapsed":3687,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"b82c9065-ef51-424f-f833-2ddd062e14d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.7/dist-packages (0.2.0)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MRbq9u6vqON","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649789389921,"user_tz":-330,"elapsed":12,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"2ad60d26-814c-49d0-ba3e-6c46201eee0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk.corpus\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","from nltk.corpus import stopwords, words\n","from nltk import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from gensim.models import Word2Vec\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import scale, MinMaxScaler\n","from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, recall_score, precision_score\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from sklearn.model_selection import learning_curve\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.multioutput import MultiOutputClassifier\n","from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"]},{"cell_type":"markdown","source":["# Pre processing + Word2Vec"],"metadata":{"id":"xFhwwRDD4n6j"}},{"cell_type":"code","source":["test_label = pd.read_csv(\"test_labels.csv\")\n","test = pd.read_csv(\"test.csv\")\n","train = pd.read_csv(\"train.csv\")\n","# Merging test and train to form one huge dataset\n","test_data = pd.merge(test, test_label)\n","dataset = pd.concat([test_data, train])\n","dataset.drop(columns=['id'], inplace=True)\n","dataset.drop_duplicates(inplace=True, ignore_index=True)\n","dataset.drop(dataset.index[dataset['toxic'] == -1], inplace = True)\n","dataset.reset_index(inplace = True)\n","# Text cleaning\n","#converting to lower case\n","dataset['comment_text_cleaned'] = dataset['comment_text'].str.lower()\n","#removing special characters\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \" \", str(elem)))\n","#removing numbers\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda elem: re.sub(r\"\\d+\", \"\", str(elem)))\n","# Removing stop words\n","stop = stopwords.words('english')\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n","#Tokenizing\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_tokenize(x))\n","#Lemmitization\n","def word_lemmatizer(text):\n","    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]\n","    return lem_text\n","dataset['comment_text_cleaned'] = dataset['comment_text_cleaned'].apply(lambda x: word_lemmatizer(x))\n","# Splitting into train test sets\n","X = dataset.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])\n","y = dataset[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].copy()\n","\n","X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X,y, train_size=0.8)\n","X_val, X_test, y_val, y_test = train_test_split(X_test_and_val,y_test_and_val, train_size=0.5)\n","train_tokens = pd.Series(X_train['comment_text_cleaned']).values\n","w2v_model = Word2Vec(train_tokens, size= 200)\n","def buildWordVector(tokens, size):\n","  vec = np.zeros(size).reshape((1, size))\n","  count = 0.\n","  for word in tokens:\n","    try:\n","      vec += w2v_model[word].reshape((1, size))\n","      count += 1.\n","    except KeyError:\n","      continue\n","  if count != 0:\n","    vec /= count\n","  return vec\n","\n","train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in train_tokens])\n","train_vecs_w2v = scale(train_vecs_w2v)\n","\n","val_tokens = pd.Series(X_val['comment_text_cleaned']).values\n","val_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in val_tokens])\n","val_vecs_w2v = scale(val_vecs_w2v)\n","\n","def evaluation_metric(model_name, feature_extraction, y_true, y_pred):\n","  print('Model:', model_name)\n","  print('Feature extraction method:', feature_extraction)\n","  recall= recall_score(y_true, y_pred, average='micro')\n","  prec = precision_score(y_true, y_pred, average='micro')\n","  final_score = recall*0.6 + prec*0.4\n","  print('Precision: ', prec)\n","  print('Recall: ', recall)\n","  print('Final score of the model: ', final_score)\n","  return final_score"],"metadata":{"id":"buTxy5nu4cWY","executionInfo":{"status":"ok","timestamp":1649789675657,"user_tz":-330,"elapsed":285742,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bafe6414-e246-461d-9a7b-41e05eea674e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"]}]},{"cell_type":"markdown","source":["# Grid Search - Naive Bayes"],"metadata":{"id":"ibRPf66-41rm"}},{"cell_type":"code","source":["parameters = [\n","    {\n","        'classifier': [GaussianNB()],\n","        'classifier__var_smoothing': np.logspace(0,-9, num=100),\n","    }\n","]\n","\n","params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}\n","\n","clf = GridSearchCV(ClassifierChain(), parameters, scoring=\"f1_micro\", return_train_score=True)\n","clf.fit(train_vecs_w2v, y_train)\n","clf.best_estimator_.fit(train_vecs_w2v, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMljtyjLBMGf","executionInfo":{"status":"ok","timestamp":1649794867655,"user_tz":-330,"elapsed":5192002,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"bb54c332-de47-4b1b-8d1f-0f9207301d35"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ClassifierChain(classifier=GaussianNB(var_smoothing=1.0),\n","                require_dense=[True, True])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["print(\"Optimal hyperparameter combination:\", clf.best_params_)\n","print()\n","print(\"Mean cross-validated training accuracy score:\", clf.best_score_)\n","predictions = clf.best_estimator_.predict(val_vecs_w2v) # Predictions\n","predictions_train = clf.best_estimator_.predict(train_vecs_w2v)\n","\n","result_test = evaluation_metric('Support Vector Machine with Classifier Chains', 'Word2Vec', y_val, predictions)\n","result_train = evaluation_metric('Random Forest with Multi Output classifier', 'Word2Vec', y_train, predictions_train)\n","print(\"Variance is: \",result_train - result_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0rec7dWVBsv5","executionInfo":{"status":"ok","timestamp":1649794873917,"user_tz":-330,"elapsed":6267,"user":{"displayName":"Aditi Gupta","userId":"12028820555348614485"}},"outputId":"e2ab3e6f-62aa-45eb-f64e-86122b05244d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Optimal hyperparameter combination: {'classifier': GaussianNB(var_smoothing=1.0), 'classifier__var_smoothing': 1.0}\n","\n","Mean cross-validated training accuracy score: 0.399933802123833\n","Model: Support Vector Machine with Classifier Chains\n","Feature extraction method: Word2Vec\n","Precision:  0.2766789328426863\n","Recall:  0.7308626974483596\n","Final score of the model:  0.5491891916060903\n","Model: Random Forest with Multi Output classifier\n","Feature extraction method: Word2Vec\n","Precision:  0.27607027347712093\n","Recall:  0.7266339251983377\n","Final score of the model:  0.546408464509851\n","Variance is:  -0.0027807270962392217\n"]}]}]}