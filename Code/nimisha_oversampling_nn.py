# -*- coding: utf-8 -*-
"""Nimisha_Oversampling_NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NTmJiejelzjoAI8KjhnTZLcuPkEaxkRW

## Installing libraries
"""

!pip install scikit-multilearn
!pip install contractions
# !pip install mlxtend

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlxtend --upgrade

import pandas as pd
import numpy as np
import re
import nltk.corpus
nltk.download('stopwords')
nltk.download('words')
nltk.download('punkt')
nltk.download('wordnet')
from nltk.corpus import stopwords, words
from nltk import word_tokenize
from nltk.stem import WordNetLemmatizer
import contractions
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import scale, MinMaxScaler
from sklearn.metrics import accuracy_score, recall_score, precision_score
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from mlxtend.evaluate import bias_variance_decomp

"""## Importing Dataset

"""

test_label = pd.read_csv("test_labels.csv")
test = pd.read_csv("test.csv")
train = pd.read_csv("train.csv")

# Merging test and train to form one huge dataset
test_data = pd.merge(test, test_label)
dataset = pd.concat([test_data, train])
dataset.drop(columns=['id'], inplace=True)
dataset.drop_duplicates(inplace=True, ignore_index=True)
dataset.drop(dataset.index[dataset['toxic'] == -1], inplace = True)
dataset.reset_index(inplace = True, drop=True)
dataset

"""## Feature Engineering """

# Comment length
dataset['length'] = dataset.comment_text.apply(lambda x: len(x))
    
# Capitalization percentage
def pct_caps(s):
  return sum([1 for c in s if c.isupper()]) / (sum(([1 for c in s if c.isalpha()])) + 1)

dataset['caps'] = dataset.comment_text.apply(lambda x: pct_caps(x))

# Mean Word length 
def word_length(s):
  s = s.split(' ')
  return np.mean([len(w) for w in s if w.isalpha()])

dataset['word_length'] = dataset.comment_text.apply(lambda x: word_length(x))

# Average number of exclamation points 
dataset['exclamation'] = dataset.comment_text.apply(lambda s: len([c for c in s if c == '!']))

# Average number of question marks 
dataset['question'] = dataset.comment_text.apply(lambda s: len([c for c in s if c == '?']))
    
# Normalize
for label in ['length', 'caps', 'word_length', 'question', 'exclamation']:
  minimum = dataset[label].min()
  diff = dataset[label].max() - minimum
  dataset[label] = dataset[label].apply(lambda x: (x-minimum) / (diff))

# Strip IP Addresses
ip = re.compile('(([2][5][0-5]\.)|([2][0-4][0-9]\.)|([0-1]?[0-9]?[0-9]\.)){3}'
                    +'(([2][5][0-5])|([2][0-4][0-9])|([0-1]?[0-9]?[0-9]))')
def strip_ip(s, ip):
  try:
    found = ip.search(s)
    return s.replace(found.group(), ' ')
  except:
    return s

dataset.comment_text = dataset.comment_text.apply(lambda x: strip_ip(x, ip))
    
dataset

"""## Text cleaning"""

# Text cleaning
#converting to lower case
#dataset['comment_text_cleaned'] = dataset['comment_text'].str.lower()
#removing special characters
dataset['comment_text'] = dataset['comment_text'].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", " ", str(elem)))
#removing numbers
dataset['comment_text'] = dataset['comment_text'].apply(lambda elem: re.sub(r"\d+", "", str(elem)))
# Removing stop words
stop = stopwords.words('english')
dataset['comment_text'] = dataset['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
# Replacing contractions with their full forms
dataset['comment_text'] = dataset['comment_text'].apply(lambda x: contractions.fix(x))
#Tokenizing
dataset['comment_text'] = dataset['comment_text'].apply(lambda x: word_tokenize(x))
#Lemmitization
def word_lemmatizer(text):
    lem_text = [WordNetLemmatizer().lemmatize(i) for i in text]
    return lem_text
dataset['comment_text'] = dataset['comment_text'].apply(lambda x: word_lemmatizer(x))

# Splitting into train test sets
X = dataset.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])
y = dataset[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].copy()

X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X,y, train_size=0.8)
X_val, X_test, y_val, y_test = train_test_split(X_test_and_val,y_test_and_val, train_size=0.5)

y_train

dataset2= X_train.join(y_train)
# dataset2=dataset2.drop(columns=['level_0'])

dataset2

X_train.reset_index(drop=True)

dd = dataset2.loc[dataset2['severe_toxic'] == 1]
dataset2= pd.concat([dataset2, dd], ignore_index=True)
dd= dataset2.loc[dataset2['threat'] == 1]
dataset2= pd.concat([dataset2, dd], ignore_index=True)
dd= dataset2.loc[dataset2['identity_hate'] == 1]
dataset2= pd.concat([dataset2, dd], ignore_index=True)

X_train = dataset2.drop(columns=['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'])
y_train = dataset2[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].copy()

#dd = X_train.loc[dataset['severe_toxic'] == 1]
#X_train= pd.concat([dataset, dd])
#dd= X_train.loc[dataset['threat'] == 1]
#X_train= pd.concat([dataset, dd])
#dd= X_train.loc[dataset['identity_hate'] == 1]
#X_train= pd.concat([dataset, dd])

train_tokens = pd.Series(X_train['comment_text']).values
w2v_model = Word2Vec(train_tokens, size= 200)

def buildWordVector(tokens, size):
  vec = np.zeros(size).reshape((1, size))
  count = 0.
  for word in tokens:
    try:
      vec += w2v_model[word].reshape((1, size))
      count += 1.
    except KeyError:
      continue
  if count != 0:
    vec /= count
  return vec

train_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in train_tokens])
# train_vecs_w2v = scaler.fit_transform(train_vecs_w2v)
train_vecs_w2v = scale(train_vecs_w2v)

val_tokens = pd.Series(X_val['comment_text']).values
val_vecs_w2v = np.concatenate([buildWordVector(z, 200) for z in val_tokens])
# val_vecs_w2v = scaler.transform(val_vecs_w2v)
val_vecs_w2v = scale(val_vecs_w2v)

train_vecs_w2v.shape

X_train

train_df = pd.DataFrame(train_vecs_w2v)
X_train = X_train.reset_index()
# train_df=X_train
# train_df=train_df.drop(columns=['index','comment_text'])
train_df['length']= X_train['length']
train_df['caps']=X_train['caps']
train_df['word_length']= X_train['word_length']
train_df['exclamation']=X_train['exclamation']
train_df['question']=X_train['question']
train_df= train_df.fillna(0)
train_df

val_df = pd.DataFrame(val_vecs_w2v)
# val_df=X_val
# val_df=val_df.drop(columns=['index','comment_text'])
X_val = X_val.reset_index()

val_df['length']= X_val['length']
val_df['caps']=X_val['caps']
val_df['word_length']= X_val['word_length']
val_df['exclamation']=X_val['exclamation']
val_df['question']=X_val['question']
val_df= val_df.fillna(0)

val_df

"""## Neural NETWORK"""

model = Sequential()
model.add(Dense(160, input_dim = train_df.shape[1], activation='sigmoid', kernel_initializer='he_uniform'))
model.add(Dense(y_train.shape[1], activation='sigmoid'))
model.compile(optimizer='adam',loss='binary_crossentropy',)
model.fit(train_df, y_train, epochs=10, verbose=0) #epochs=100

predictions = model.predict(val_df)
predictions = predictions.round()
predictions

import pickle as pk
pk.dump(model, open('tuned_neural_network_model.sav', 'wb'))

"""## Evaluation"""

def score(y_true, y_pred, label):
  prec = precision_score(y_true, y_pred,average='micro')
  re = recall_score(y_true, y_pred,average='micro')
  print('Results for label:', label)
  print('Precision score:', prec)
  print('Recall score:', re)
  print('Final Score:', re*0.6 + prec*0.4, '\n')
  return re*0.6 + prec*0.4

val_score = score(y_val, predictions,'Neural Network with Word2Vec')

#Variance
predictions1 = model.predict(train_df)
predictions1 = predictions1.round()

train_Score = score(y_train, predictions1,'Neural Network with Word2Vec')
print("The variance for neural network is:")
print(train_Score-val_score)

predictions = pd.DataFrame({'toxic': predictions[:, 0], 'severe_toxic': predictions[:, 1], 'obscene': predictions[:, 2], 'threat': predictions[:, 3], 'insult': predictions[:, 4], 'identity_hate': predictions[:, 5]})

print('Model: Tuned NN with \nFeature extraction method: Word2Vec ')
print()
prec_score = []
re_score = []
fina_score = []
labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']

for label in labels:
  scores = score(y_val[label], predictions[label], label)
  fina_score.append(scores[0])
  prec_score.append(scores[1])
  re_score.append(scores[2])

from matplotlib.legend_handler import HandlerLine2D
line1, = plt.plot(labels, fina_score, 'b', label='Final score')
line2, = plt.plot(labels, prec_score, 'r', label='Precision score')
line3, = plt.plot(labels, re_score, 'g', label='Recall score')
plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})
plt.ylabel('Score')
plt.xlabel('Labels')
plt.show()